{"meta":{"title":"Eric Websmith's Blog","subtitle":"Machine Learning","description":"","author":"Eric Websmith","url":"https://EricWebsmith.github.io","root":"/"},"pages":[{"title":"about","date":"2020-04-03T16:33:45.000Z","updated":"2020-04-03T16:34:38.715Z","comments":true,"path":"about/index.html","permalink":"https://ericwebsmith.github.io/about/index.html","excerpt":"","text":"This is all about me."}],"posts":[{"title":"Bayesian Optimization","slug":"BayesianOptimization","date":"2020-04-11T02:59:48.000Z","updated":"2020-04-11T05:58:15.417Z","comments":true,"path":"2020/04/11/BayesianOptimization/","link":"","permalink":"https://ericwebsmith.github.io/2020/04/11/BayesianOptimization/","excerpt":"","text":"According to Wikipedia, Bayesian optimization (BO) is a sequential design strategy for global optimization of black-box functions that do not require derivatives. Study ObjectiveIn this post, we will: Review optimization algorithms. Compare Bayesian optimization with gradient descent. Understand the process of Bayesian optimization. Write a simple Bayesian optimization algorithm from scratch. Use BO not from scratch. OptimizationOptimization is to find the global max or min from the function. Besides BO, we also use grid search, random search and gradient descent. The first two can be used on any function if computation power is not the problem. The third is used only if the function is convex and derivative. Bayesian optimization has no such limitations. However, the function is supposed to be smooth and continuous, otherwise, I would recommend grid search and random search. ProcessI think the whole process can be demonstrated as the following trinity: Here the objective function is the black function that we optimize. It produces samples (x, y). Usually, it is very costly to run the objective function, such that we introduce a surrogate function to predict the objective function. The prediction is mean and standard deviation of the objective function, which are used by the acquisition function in searching for the next x to explore or exploit. The next x is then consumed by the objective function. The loop goes on until the max or min is found. The surrogate function is usually the Gaussian process regressor while the acquisition function has many options. They are: Probability of Improvement (PI). Expected Improvement (EI). Upper/Lower Confidence Bound (LCB/UCB). Here we only focus on what we use, the UCB, the most straightforward one. If we ignore weight, UCB can be written as: ùëàùê∂ùêµ(ùë•)=ùúá(ùë•)+ùúé(ùë•) ùë•=argmax ùëàùê∂ùêµ(ùë•) We do all of this inside the search space. The whole process: Initiate x with the min and max of the search space. Calculate y using x and objective function. Fit the surrogate function. Find new x using the acquisition function. Go to 1 Python CodeLet‚Äôs first define and plot the objective function as following: 12def objective(x): return ((x-0.47)**2 * math.sin(3 * x)) Here the maximum (x=0.87, y= 0.0811051) is also plotted as a blue point. Now, this function is non-convex, thus gradient descent cannot be used. We use BO. The surrogate function is the Gaussian process regressor and UCB is used as the acquisition function. 12345678910111213#uppper confidence bound (UCB)#beta = 1def acquisition(mean, std): mean=mean.flatten() #UCB upper=mean+std #argmax max_at=np.argmax(upper) return X[max_at]#surrogatesurrogate = GaussianProcessRegressor() Code for the whole process 123456789101112131415161718192021#step 0 Initiate x with the min and max of the search space.xsamples=np.array([[0],[1]])#step 1 Calculate y using x and objective function.ysamples=np.array([objective(x) for x in xsamples])for i in range(4): #step 2 Fit the surrogate function. surrogate.fit(xsamples, ysamples) mean, std=surrogate.predict(X, return_std=True) #step 3 Find new x using acquisition function. new_x=acquisition(mean, std) #step 4 Go to 1 new_y=objective(new_x) #plot plot(X, y, xsamples, ysamples, mean, std, new_x, new_y, i) xsamples=np.vstack((xsamples, new_x)) ysamples=np.vstack((ysamples, new_y)) Step 0: In step 0, we simply produce two sample x=0 and x=1. Iteration 0: In iteration 0, the two samples are used by the surrogate function to generate mean and std(green). The acquisition function finds the max UCB when x=0.53(red). Objective(0.53) is 0.00359934. Now a new point (0.53, 0.00359934) is found. We give the three points to iteration 1. Iteration 1: Iteration 2: Iteration 3: The new point(red) here is (0.53, 0.00359934) and it is just the max. We can see that BO can find the optimum in just 4 iterations. And we call the objective function only 6 times. If we use the grid search, that will be 100 times. The complete version of the code can be found here: https://github.com/EricWebsmith/machine_learning_from_scrach/blob/master/bayesian_optimization.ipynb Bayesian Optimization not from scratchThere are many tools for BO. One of them is Hyperopt. The following is just a simple demonstration of that. 1234567from hyperopt import fmin, tpe, hpbest = fmin( fn=lambda x:-objective(x), space=hp.uniform('x', 0, 1), algo=tpe.suggest, max_evals=100)print(best) Referencehttps://en.wikipedia.org/wiki/Bayesian_optimization","categories":[],"tags":[{"name":"AutoML","slug":"AutoML","permalink":"https://ericwebsmith.github.io/tags/AutoML/"},{"name":"Optimization","slug":"Optimization","permalink":"https://ericwebsmith.github.io/tags/Optimization/"}]},{"title":"AutoML Introduction","slug":"automl","date":"2020-04-03T16:29:50.000Z","updated":"2020-04-11T05:58:53.480Z","comments":true,"path":"2020/04/04/automl/","link":"","permalink":"https://ericwebsmith.github.io/2020/04/04/automl/","excerpt":"","text":"With the development of machine learning, demand for AutoML (Automatic Machine Learning) increases. This article split this topic into two, namely AutoML in classic machine learning and that in deep learning. After completing this tutorial, you will know: AutoML in both classic and deep machine learning. A peek of two open-source frameworks, Auto-Sklearn and Auto-Keras. Two algorithms, Bayesian Optimization and Network Morphism. The ideas behind Neural Architecture Search. AutoML in Classic Machine LearningCASHAutoML has many sub-functionalities, including automatic data analysis, automatic feature engineering, automatic normalization and regularization, automatic feature selection, automatic algorithm selection and automatic hyperparameter optimization(HPO). Recent studies focus on the CASH[1] problem. That is Combined Algorithm and Selection and Hyperparameter Optimization. CASH was proposed by Thornton in Auto-Weka[1]. The solution he provided was Bayesian Optimization[2]. Bayesian OptimizationBayesian Optimization (BO) is one of the means we find extrema (both max and min values) in a function. Other options may include grid search, random search and gradient descent. Compared to gradient descent, BO optimizes functions that are non-convex and non-derivative. In BO, we call the black-box function we optimize the objective function. This function is usually costly to run. Thus, we introduce the surrogate function to estimate the mean and standard deviation of the objective function. The output is then used by the acquisition function in searching for the next sample to exploit or explore. The BO searches for extrema in a finite search space. The process is summarized as follow: Sample some points x in the search space. Find the corresponding y by the objective function. update the surrogate function using x, y. find the next x using the acquisition function. Goto 2 unless finding extrema. The following flowchart demonstrates the process. Now let‚Äôs look at an example. Suppose we have the following function: The search space is [0, 1]. Firstly two points x=0 and x=1 are selected, and their y values are calculated using the objective function. Then the mean and standard deviation are generated by the Gaussian process or the surrogate function. The UCB (Upper Confidence Bound) algorithm is used as the acquisition function. It simply takes the max value of the upper confidence bound (red in the plot) as the next sample point. The BO continues‚Ä¶ After four iterations, the max value is found. The above pictures were generated by a notebook from my Github Repo Machine Learning Step by Step. Auto-SklearnAuto-Sklearn[3] used Bayesian Optimization to find the best hyperparameters for machine learning. Besides that, it proposed two new improvements. The first is meta-learning. Auto-Sklearn collected several datasets, extracted features from these datasets. These datasets are trained and the best hyperparameters are collected. The dataset features and best hyperparameters are treated as input and output for machine learning and the relationship is learned. Thus, the search space is narrow and the algorithm runs faster. The second improvement is ensemble. As we all know ensemble improves accuracy. I installed Auto-Sklearn. The Titanic Survive and Boston House Price datasets are used. Each took one hour to finish on an i7 CPU. The scores are 0.76 and 0.85. Not too bad. Auto-Sklearn focuses on the CASH problem. it does not handle data preprocessing. I have to remove string features like names and tickets in advance. Categorical features like embarking place, gender have to be transformed into Boolean values or one-hot values. Sample code for auto-sklearn 1234import autosklearn.classificationcls = autosklearn.classification.AutoSklearnClassifier()cls.fit(X_train, y_train)predictions = cls.predict(X_test) Official Website: https://automl.github.io/auto-sklearn/master/ Deep Learning in AutoMLNeural Architecture SearchNeural Architecture Search (NAS) is the main research topic in deep learning automation. The rough idea is to treat deep models as strings such that the next layer can be predicted by RNN. NAS named this RNN the controller. The original NAS used reinforcement learning to optimize the parameters. Some architectures have branches. NAS uses a batch of bi-classifiers to determine if Layer N connects to one of the N-1 layers. The following are the original LSTM (upper left) and two LSTM structures NAS found. Xin He[5] summarized all the NAS works. For controllers, grid search, random search, reinforcement learning, evolution algorithm, Bayesian optimization, gradient descent are used. Low fidelity, transfer learning, surrogate, early stopping are used to perform more efficiently. The following is a comparison of all the NAS implementations: Network MorphismIn the process of NAS, a great number of architectures are tested. If the new network can learn parameters from the old ones, it would be more efficient. Network Morphism[6] provides such a mechanism. Network Morphism named the old and new network the parent network and the child network. The child network inherits knowledge from the parent network. There are five cases in Network Morphism, linear, non-linear, stand-alone width, stand-alone kernal, and subnet. Linear Case A new layer, Layer L is inserted between Layer L-1 and Layer L+1. Bl-1 and Bl+1 are two matrices and Bl+1=Bl-1 * G. To insert a new layer, we simply split G into two matrices, Fl and Fl+1 . Non-Linear Case To insert a non-linear layer between two layers, a new activation function, parametric activation is introduced. It starts as an Identity matrix and learns to become an activation function afterward. It is a wrapper around other activation functions. The equation is as follow: Stand-alone Width We calculate the equations from the above chart. We finally will have the following: Kernal For kernals, we simply do padding. Subnet Firstly, we insert the layers one by one. this is sequential subnet morphing. We then stack them by simply split G into a group of G, each is G/n. Auto-KerasAuto-Keras is an opensource based on Keras. It uses Bayesian optimization and network morphism. It trains CNN, RNN and traditional DNN. Sample code: 12345import autokeras as akclf = ak.ImageClassifier()clf.fit(x_train, y_train)results = clf.predict(x_test) The official website: https://autokeras.com/ Where to use AutoML?AutoML is suitable in some mature scenarios, like traditional ML, image recognition, object detection, OCR, text analysis, etc.. The reason is that we have limit options. For problems like audio-related research, because it is not mature, what is to try is not clear. Thus it is not possible to use autoML. Reference[1] C. Thornton, F. Hutter, H. Hoos, and K. Leyton-Brown. Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms. In Proc. of KDD‚Äô13, pages 847‚Äì855, 2013. [2] E. Brochu, V. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, abs/1012.2599, 2010. [3] Efficient and Robust Automated Machine Learning, Feurer et al., Advances in Neural Information Processing Systems 28 (NIPS 2015). [4] Zoph, Barret, and Quoc V. Le. ‚ÄúNeural Architecture Search with Reinforcement Learning.‚Äù arXiv: Learning (2016). [5] He, Xin, Kaiyong Zhao, and Xiaowen Chu. ‚ÄúAutoML: A Survey of the State-of-the-Art..‚Äù arXiv: Learning (2019). [6] Wei, Tao, et al. ‚ÄúNetwork morphism.‚Äù international conference on machine learning (2016): 564-572.","categories":[],"tags":[{"name":"AutoML","slug":"AutoML","permalink":"https://ericwebsmith.github.io/tags/AutoML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://ericwebsmith.github.io/tags/Deep-Learning/"}]}],"categories":[],"tags":[{"name":"AutoML","slug":"AutoML","permalink":"https://ericwebsmith.github.io/tags/AutoML/"},{"name":"Optimization","slug":"Optimization","permalink":"https://ericwebsmith.github.io/tags/Optimization/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://ericwebsmith.github.io/tags/Deep-Learning/"}]}